{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKiTCJQXCm6efwYdHhCMtr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anggaangoro3/data-science-project/blob/main/perceptron_atau_varian_svm_linear_dasar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdmsyQGJ11jH",
        "outputId": "00ec632c-e103-4fb3-f02e-ed6e42359662"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w [0.45527548 1.64412317]\n",
            "Epoch 0: Loss = 6.6431\n",
            "w [0.43327548 1.61612317]\n",
            "w [0.41127548 1.58812317]\n",
            "w [0.38927548 1.56012317]\n",
            "w [0.36727548 1.53212317]\n",
            "w [0.34527548 1.50412317]\n",
            "w [0.32327548 1.47612317]\n",
            "w [0.30127548 1.44812317]\n",
            "w [0.27927548 1.42012317]\n",
            "w [0.25727548 1.39212317]\n",
            "w [0.23527548 1.36412317]\n",
            "w [0.21327548 1.33612317]\n",
            "w [0.19127548 1.30812317]\n",
            "w [0.16927548 1.28012317]\n",
            "w [0.14727548 1.25212317]\n",
            "w [0.12527548 1.22412317]\n",
            "w [0.10327548 1.19612317]\n",
            "w [0.08127548 1.16812317]\n",
            "w [0.05927548 1.14012317]\n",
            "w [0.03727548 1.11212317]\n",
            "Training selesai!\n",
            "Bobot (w): [0.01527548 1.08412317]\n",
            "Bias (b): 0.6099550535114413\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Definisikan data\n",
        "X = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [4, 5]])  # Contoh fitur\n",
        "y = np.array([1, 1, -1, -1, -1])  # Contoh label dua kelas\n",
        "\n",
        "# Inisialisasi bobot dan bias secara acak\n",
        "w = np.random.randn(X.shape[1])  # Inisialisasi bobot\n",
        "# [x1, w1] + [x2, w2]\n",
        "b = np.random.randn()  # Inisialisasi bias\n",
        "\n",
        "# Definisikan learning rate\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Definisikan jumlah iterasi\n",
        "epochs = 20\n",
        "\n",
        "# Lakukan training\n",
        "for epoch in range(epochs):\n",
        "    # Hitung prediksi\n",
        "    # np.dot(X, w) # X.W\n",
        "    scores = np.dot(X, w) + b\n",
        "    print('w',w)\n",
        "    # print('scores', scores)\n",
        "\n",
        "    # Hitung loss dengan menggunakan hinge loss\n",
        "    margins = y * scores\n",
        "\n",
        "    # print('margins', margins)\n",
        "    loss = np.maximum(0, 1 - margins) # hinge loss\n",
        "    # print('loss', loss)\n",
        "    mean_loss = np.mean(loss)\n",
        "\n",
        "    # Hitung gradien\n",
        "    grad_w = np.zeros_like(w)\n",
        "    # print('grad_w', grad_w)\n",
        "    grad_b = 0\n",
        "    for i, margin in enumerate(margins):\n",
        "        if margin < 1:\n",
        "            grad_w += -y[i] * X[i]\n",
        "            # print('-y[i] * X[i]',-y[i] * X[i])\n",
        "            grad_b += -y[i]\n",
        "            # print('grad_b',grad_b)\n",
        "    grad_w /= len(X)\n",
        "    grad_b /= len(X)\n",
        "\n",
        "    # Update bobot dan bias menggunakan gradien turun\n",
        "    w -= learning_rate * grad_w\n",
        "    b -= learning_rate * grad_b\n",
        "\n",
        "    # Tampilkan loss setiap 100 iterasi\n",
        "    if epoch % 100 == 0:\n",
        "        print(f'Epoch {epoch}: Loss = {mean_loss:.4f}')\n",
        "\n",
        "print('Training selesai!')\n",
        "print('Bobot (w):', w)\n",
        "print('Bias (b):', b)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Definisikan data (fitur dan label)\n",
        "X = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [4, 5]])  # Contoh fitur\n",
        "y = np.array([1, 1, -1, -1, -1])  # Contoh label dua kelas (-1 dan 1)\n",
        "\n",
        "# Inisialisasi bobot dan bias secara acak\n",
        "np.random.seed(42)  # Agar hasil random tetap sama\n",
        "w = np.random.randn(X.shape[1])  # Bobot\n",
        "print(w)\n",
        "b = np.random.randn()  # Bias\n",
        "\n",
        "# Hyperparameter\n",
        "learning_rate = 0.01\n",
        "epochs = 15000  # Jumlah iterasi\n",
        "reg_lambda = 0.01  # Faktor regulasi untuk SVM\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    # Hitung prediksi\n",
        "    scores = np.dot(X, w) + b\n",
        "    margins = y * scores  # Hinge loss margin\n",
        "    loss = np.maximum(0, 2 - margins)  # Hinge loss\n",
        "\n",
        "    # Gradien bobot dan bias\n",
        "    grad_w = np.zeros_like(w)\n",
        "    grad_b = 0\n",
        "\n",
        "    for i, margin in enumerate(margins):\n",
        "        if margin < 1:  # Hanya hitung jika ada pelanggaran margin\n",
        "            grad_w += -y[i] * X[i]\n",
        "            grad_b += -y[i]\n",
        "\n",
        "    # Normalisasi gradien\n",
        "    grad_w /= len(X)\n",
        "    grad_b /= len(X)\n",
        "\n",
        "    # Tambahkan regularisasi L2 untuk bobot\n",
        "    grad_w += reg_lambda * w\n",
        "\n",
        "    # Update bobot dan bias\n",
        "    w -= learning_rate * grad_w\n",
        "    b -= learning_rate * grad_b\n",
        "\n",
        "    # Tampilkan loss setiap 100 iterasi\n",
        "    if epoch % 100 == 0:\n",
        "        mean_loss = np.mean(loss)\n",
        "        print(f'Epoch {epoch}: Loss = {mean_loss:.4f}')\n",
        "\n",
        "print('\\nTraining selesai!')\n",
        "print('Bobot (w):', w)\n",
        "print('Bias (b):', b)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTJQ5-4a-_UT",
        "outputId": "1bcb8c46-c7f7-4ca0-8888-122f83621890"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.49671415 -0.1382643 ]\n",
            "Epoch 0: Loss = 2.6754\n",
            "Epoch 100: Loss = 1.4851\n",
            "Epoch 200: Loss = 1.4551\n",
            "Epoch 300: Loss = 1.4259\n",
            "Epoch 400: Loss = 1.3975\n",
            "Epoch 500: Loss = 1.3075\n",
            "Epoch 600: Loss = 1.2807\n",
            "Epoch 700: Loss = 1.2546\n",
            "Epoch 800: Loss = 1.1157\n",
            "Epoch 900: Loss = 0.9782\n",
            "Epoch 1000: Loss = 0.8423\n",
            "Epoch 1100: Loss = 0.7913\n",
            "Epoch 1200: Loss = 0.7791\n",
            "Epoch 1300: Loss = 0.7650\n",
            "Epoch 1400: Loss = 0.7529\n",
            "Epoch 1500: Loss = 0.7408\n",
            "Epoch 1600: Loss = 0.7267\n",
            "Epoch 1700: Loss = 0.7146\n",
            "Epoch 1800: Loss = 0.7026\n",
            "Epoch 1900: Loss = 0.6906\n",
            "Epoch 2000: Loss = 0.6766\n",
            "Epoch 2100: Loss = 0.6646\n",
            "Epoch 2200: Loss = 0.6526\n",
            "Epoch 2300: Loss = 0.6407\n",
            "Epoch 2400: Loss = 0.6288\n",
            "Epoch 2500: Loss = 0.6149\n",
            "Epoch 2600: Loss = 0.6030\n",
            "Epoch 2700: Loss = 0.5911\n",
            "Epoch 2800: Loss = 0.5793\n",
            "Epoch 2900: Loss = 0.5674\n",
            "Epoch 3000: Loss = 0.5556\n",
            "Epoch 3100: Loss = 0.5438\n",
            "Epoch 3200: Loss = 0.5300\n",
            "Epoch 3300: Loss = 0.5183\n",
            "Epoch 3400: Loss = 0.5065\n",
            "Epoch 3500: Loss = 0.4994\n",
            "Epoch 3600: Loss = 0.4952\n",
            "Epoch 3700: Loss = 0.4911\n",
            "Epoch 3800: Loss = 0.4870\n",
            "Epoch 3900: Loss = 0.4830\n",
            "Epoch 4000: Loss = 0.4790\n",
            "Epoch 4100: Loss = 0.4750\n",
            "Epoch 4200: Loss = 0.4710\n",
            "Epoch 4300: Loss = 0.4671\n",
            "Epoch 4400: Loss = 0.4633\n",
            "Epoch 4500: Loss = 0.4595\n",
            "Epoch 4600: Loss = 0.4557\n",
            "Epoch 4700: Loss = 0.4519\n",
            "Epoch 4800: Loss = 0.4482\n",
            "Epoch 4900: Loss = 0.4446\n",
            "Epoch 5000: Loss = 0.4409\n",
            "Epoch 5100: Loss = 0.4374\n",
            "Epoch 5200: Loss = 0.4338\n",
            "Epoch 5300: Loss = 0.4303\n",
            "Epoch 5400: Loss = 0.4268\n",
            "Epoch 5500: Loss = 0.4234\n",
            "Epoch 5600: Loss = 0.4172\n",
            "Epoch 5700: Loss = 0.4138\n",
            "Epoch 5800: Loss = 0.4105\n",
            "Epoch 5900: Loss = 0.4072\n",
            "Epoch 6000: Loss = 0.4019\n",
            "Epoch 6100: Loss = 0.3999\n",
            "Epoch 6200: Loss = 0.3967\n",
            "Epoch 6300: Loss = 0.3955\n",
            "Epoch 6400: Loss = 0.3984\n",
            "Epoch 6500: Loss = 0.3984\n",
            "Epoch 6600: Loss = 0.3980\n",
            "Epoch 6700: Loss = 0.3980\n",
            "Epoch 6800: Loss = 0.3980\n",
            "Epoch 6900: Loss = 0.3981\n",
            "Epoch 7000: Loss = 0.3981\n",
            "Epoch 7100: Loss = 0.3981\n",
            "Epoch 7200: Loss = 0.3981\n",
            "Epoch 7300: Loss = 0.3982\n",
            "Epoch 7400: Loss = 0.3970\n",
            "Epoch 7500: Loss = 0.3962\n",
            "Epoch 7600: Loss = 0.3982\n",
            "Epoch 7700: Loss = 0.3954\n",
            "Epoch 7800: Loss = 0.3975\n",
            "Epoch 7900: Loss = 0.3979\n",
            "Epoch 8000: Loss = 0.3979\n",
            "Epoch 8100: Loss = 0.3979\n",
            "Epoch 8200: Loss = 0.3984\n",
            "Epoch 8300: Loss = 0.3996\n",
            "Epoch 8400: Loss = 0.3976\n",
            "Epoch 8500: Loss = 0.3976\n",
            "Epoch 8600: Loss = 0.3981\n",
            "Epoch 8700: Loss = 0.3981\n",
            "Epoch 8800: Loss = 0.3989\n",
            "Epoch 8900: Loss = 0.3977\n",
            "Epoch 9000: Loss = 0.3977\n",
            "Epoch 9100: Loss = 0.3982\n",
            "Epoch 9200: Loss = 0.3982\n",
            "Epoch 9300: Loss = 0.3978\n",
            "Epoch 9400: Loss = 0.3982\n",
            "Epoch 9500: Loss = 0.3955\n",
            "Epoch 9600: Loss = 0.3979\n",
            "Epoch 9700: Loss = 0.3979\n",
            "Epoch 9800: Loss = 0.3975\n",
            "Epoch 9900: Loss = 0.3979\n",
            "Epoch 10000: Loss = 0.3956\n",
            "Epoch 10100: Loss = 0.3976\n",
            "Epoch 10200: Loss = 0.3980\n",
            "Epoch 10300: Loss = 0.3968\n",
            "Epoch 10400: Loss = 0.3977\n",
            "Epoch 10500: Loss = 0.3981\n",
            "Epoch 10600: Loss = 0.3977\n",
            "Epoch 10700: Loss = 0.3981\n",
            "Epoch 10800: Loss = 0.3954\n",
            "Epoch 10900: Loss = 0.3982\n",
            "Epoch 11000: Loss = 0.3954\n",
            "Epoch 11100: Loss = 0.3978\n",
            "Epoch 11200: Loss = 0.3974\n",
            "Epoch 11300: Loss = 0.3983\n",
            "Epoch 11400: Loss = 0.3975\n",
            "Epoch 11500: Loss = 0.3979\n",
            "Epoch 11600: Loss = 0.3975\n",
            "Epoch 11700: Loss = 0.3956\n",
            "Epoch 11800: Loss = 0.3976\n",
            "Epoch 11900: Loss = 0.3996\n",
            "Epoch 12000: Loss = 0.3980\n",
            "Epoch 12100: Loss = 0.3988\n",
            "Epoch 12200: Loss = 0.3981\n",
            "Epoch 12300: Loss = 0.3977\n",
            "Epoch 12400: Loss = 0.3989\n",
            "Epoch 12500: Loss = 0.3977\n",
            "Epoch 12600: Loss = 0.3954\n",
            "Epoch 12700: Loss = 0.3982\n",
            "Epoch 12800: Loss = 0.3978\n",
            "Epoch 12900: Loss = 0.3974\n",
            "Epoch 13000: Loss = 0.3983\n",
            "Epoch 13100: Loss = 0.3979\n",
            "Epoch 13200: Loss = 0.3975\n",
            "Epoch 13300: Loss = 0.3955\n",
            "Epoch 13400: Loss = 0.3979\n",
            "Epoch 13500: Loss = 0.3976\n",
            "Epoch 13600: Loss = 0.3976\n",
            "Epoch 13700: Loss = 0.3976\n",
            "Epoch 13800: Loss = 0.3980\n",
            "Epoch 13900: Loss = 0.3977\n",
            "Epoch 14000: Loss = 0.3977\n",
            "Epoch 14100: Loss = 0.3981\n",
            "Epoch 14200: Loss = 0.3981\n",
            "Epoch 14300: Loss = 0.3977\n",
            "Epoch 14400: Loss = 0.3978\n",
            "Epoch 14500: Loss = 0.3982\n",
            "Epoch 14600: Loss = 0.3982\n",
            "Epoch 14700: Loss = 0.3978\n",
            "Epoch 14800: Loss = 0.3979\n",
            "Epoch 14900: Loss = 0.3975\n",
            "\n",
            "Training selesai!\n",
            "Bobot (w): [-1.71462215 -0.29782706]\n",
            "Bias (b): 5.3236885381003285\n"
          ]
        }
      ]
    }
  ]
}